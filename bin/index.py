#!/usr/bin/env python3

# Script used to process a set of geneshot results for visualization in the GLAM Browser
import argparse
import logging
import os
import pandas as pd
import numpy as np


def parse_manifest(store):
    """Read in the manifest and filter columns for visualization."""

    # Read the whole manifest
    logging.info("Reading in the manifest")
    manifest = pd.read_hdf(store, "/manifest")
    logging.info("Read in data from {:,} rows and {:,} columns".format(
        manifest.shape[0],
        manifest.shape[1],
    ))

    # Make sure that we have a specimen column
    assert "specimen" in manifest.columns.values

    # Drop any columns with paths to reads
    for k in ["R1", "R2", "I1", "I2"]:
        if k in manifest.columns.values:
            logging.info("Removing column {}".format(k))
            manifest = manifest.drop(columns=k)

    # Drop any columns which do not have unique values for each specimen
    for k in manifest.columns.values:
        if k == "specimen":
            continue
        # Look at just the unique values for this column
        d = manifest.reindex(columns=["specimen", k]).drop_duplicates()
        # Make sure that every specimen has only one value in this column
        if d["specimen"].value_counts().max() > 1:
            logging.info("Removing column with duplicate values: {}".format(k))
            manifest = manifest.drop(columns=k)

    # Now drop duplicates and make sure the specimen column is still unique
    manifest = manifest.drop_duplicates()
    assert manifest["specimen"].value_counts().max() == 1

    # Return the filtered manifest
    return manifest


def parse_experiment_metrics(store):
    """Read in the experiment metrics"""
    return pd.read_hdf(store, "/summary/experiment")


def parse_specimen_metrics(store):
    """Read in the specimen metrics"""
    df = pd.read_hdf(store, "/summary/all")

    # Compute `prop_reads`
    return df.assign(
        prop_reads = df["aligned_reads"] / df["n_reads"]
    )


def parse_cag_annotations(store):
    """Read in the CAG annotations."""
    df = pd.read_hdf(store, "/annot/cag/all")

    # Compute `prop_reads`
    return df.assign(
        size_log10 = df["size"].apply(np.log10)
    )


def parse_cag_abundances(store):
    """Read in the CAG abundances."""
    return pd.read_hdf(store, "/abund/cag/wide")


def parse_distance_matrices(store, all_keys):
    """Read in each of the distance matrices in the store."""

    for k in all_keys:
        if k.startswith("/distances/"):
            yield k.replace("/distances/", ""), pd.read_hdf(store, k)


def index_geneshot_results(input_fp, output_fp):

    # Keep all of the data in a dict linking the key to the table
    dat = {}

    # Open a connection to the input HDF5
    with pd.HDFStore(input_fp, "r") as store:
        all_keys = store.keys()

        # Read in the manifest
        dat["/manifest"] = parse_manifest(store)

        # Read in the experiment metrics
        dat["/experiment_metrics"] = parse_experiment_metrics(store)

        # Read in the specimen metrics
        dat["/specimen_metrics"] = parse_specimen_metrics(store)

        # Read in the CAG annotations
        dat["/cag_annotations"] = parse_cag_annotations(store)

        # Read in the CAG abundances
        dat["/cag_abundances"] = parse_cag_abundances(store)

        # Read in the distance matrices
        for metric_name, metric_df in parse_distance_matrices(store, all_keys):
            dat["/distances/{}".format(metric_name)] = metric_df

    # TODO assemble the `analysis_metrics` table

    # Write out all of the tables to HDF5
    with pd.HDFStore(output_fp, "w") as store:
        for key_name, df in dat.items():
            logging.info("Writing a table with {:,} rows and {:,} columns to {}".format(
                df.shape[0], df.shape[1], output_fp
            ))
            df.to_hdf(
                store,
                key_name
            )


if __name__ == "__main__":

    log_formatter = logging.Formatter(
        "%(asctime)s %(levelname)-8s [GLAM Index] %(message)s"
    )
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # Write logs to STDOUT
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)

    parser = argparse.ArgumentParser(
        description="""
        Index a set of geneshot results for visualization with the GLAM Browser.

        Example Usage:

        index.py <INPUT_HDF_FP> <OUTPUT_HDF_FP>

        """
    )

    parser.add_argument(
        "input",
        type=str,
        help="Path to results HDF5 file generated by geneshot"
    )

    parser.add_argument(
        "output",
        type=str,
        help="Path to write out index HDF5 file which can be visualized with the GLAM Browser"
    )

    # Parse the arguments
    args = parser.parse_args()

    # Make sure the input file exists
    assert os.path.exists(args.input), "Cannot find {}".format(args.input)

    # Make sure that the output file does not exist
    assert os.path.exists(args.output) is False, "{} already exists".format(args.output)

    index_geneshot_results(
        args.input,
        args.output
    )
